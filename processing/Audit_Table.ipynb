{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f11fe8b0-e3c4-4069-ac61-7d84a52c7bc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, md5, concat_ws, lit\n",
    "from pyspark.sql.types import StringType, StructType, StructField, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "def update_audit_table(table_name, progress=None, status=None, start=False, end=False):\n",
    "    # Get the current timestamp\n",
    "    current_time = datetime.now()\n",
    "    \n",
    "    # Check if the Audit table exists\n",
    "    if not spark.catalog.tableExists(\"Audit.Audit_table\"):\n",
    "        # Create the table if it doesn't exist with timestamps\n",
    "        schema = StructType([\n",
    "            StructField(\"Table_Name\", StringType(), True),\n",
    "            StructField(\"Progress\", StringType(), True),\n",
    "            StructField(\"Status\", StringType(), True),\n",
    "            StructField(\"Start_time\", TimestampType(), True),  # Changed to TimestampType\n",
    "            StructField(\"End_time\", TimestampType(), True)      # Changed to TimestampType\n",
    "        ])\n",
    "        empty_df = spark.createDataFrame([], schema)\n",
    "        empty_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Audit.Audit_table\")\n",
    "    \n",
    "    # Define the schema explicitly to avoid inference issues\n",
    "    audit_schema = StructType([\n",
    "        StructField(\"Table_Name\", StringType(), True),\n",
    "        StructField(\"Progress\", StringType(), True),\n",
    "        StructField(\"Status\", StringType(), True),\n",
    "        StructField(\"Start_time\", TimestampType(), True),  # Timestamp for Start_time\n",
    "        StructField(\"End_time\", TimestampType(), True)      # Timestamp for End_time\n",
    "    ])\n",
    "    \n",
    "    # If start is True, insert a new record with Progress, Status, and Start_time\n",
    "    if start:\n",
    "        # Check that the necessary values are provided\n",
    "        if progress is None or status is None:\n",
    "            raise ValueError(\"Progress and Status must be provided when starting a new record.\")\n",
    "        \n",
    "        # ** No check for existing \"In_Progress\" records. ** \n",
    "        # Insert a new row with the current start time, regardless of whether an In_Progress record already exists\n",
    "        new_entry = [(table_name, progress, status, current_time, None)]\n",
    "        new_df = spark.createDataFrame(new_entry, audit_schema)\n",
    "        \n",
    "        # Append the new record to the Audit table\n",
    "        new_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"Audit.Audit_table\")\n",
    "    \n",
    "    # If end is True, update the last record with Status and End_time\n",
    "    elif end:\n",
    "        # Check that the Status is provided\n",
    "        if status is None:\n",
    "            raise ValueError(\"Status must be provided when ending a record.\")\n",
    "        \n",
    "        # Load the Audit table\n",
    "        audit_df = spark.table(\"Audit.Audit_table\")\n",
    "        \n",
    "        # Find the last record for this table_name (the most recent Start_time)\n",
    "        last_record = audit_df.filter(col(\"Table_Name\") == table_name).orderBy(col(\"Start_time\").desc()).limit(1)\n",
    "        \n",
    "        if last_record.count() == 0:\n",
    "            raise ValueError(\"No existing record found to update.\")\n",
    "        \n",
    "        # Update the last record with the new status and end time\n",
    "        audit_df = audit_df.withColumn(\n",
    "            \"Status\", when((col(\"Table_Name\") == table_name) & (col(\"Start_time\") == last_record.first()[\"Start_time\"]), status).otherwise(col(\"Status\"))\n",
    "        ).withColumn(\n",
    "            \"End_time\", when((col(\"Table_Name\") == table_name) & (col(\"Start_time\") == last_record.first()[\"Start_time\"]), lit(current_time)).otherwise(col(\"End_time\"))\n",
    "        )\n",
    "        \n",
    "        # Overwrite the Audit table with the updated DataFrame\n",
    "        audit_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"Audit.Audit_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43a8823c-eeef-4a5d-9cd4-cd4e2d1de956",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_and_store_md5(df, layer_name, table_name, schema_flag =\"N\"):\n",
    "    # Create a unique MD5 hash of all columns of the dataframe\n",
    "    columns_concatenated = concat_ws(\",\", *df.columns)\n",
    "    md5_hash = df.withColumn(\"md5_hash\", md5(columns_concatenated.cast(StringType()))).select(\"md5_hash\").first()[\"md5_hash\"]\n",
    "    \n",
    "    # Check if the metadata table exists, if not create it\n",
    "    if not spark.catalog.tableExists(\"Audit.Schema_Metadata\"):\n",
    "        # Create the metadata table if it doesn't exist\n",
    "        schema = \"Layer_Name STRING, Table_Name STRING, Schema_Flag STRING, Md5_Hash STRING\"\n",
    "        spark.sql(f\"CREATE TABLE Audit.Schema_Metadata ({schema}) USING DELTA\")\n",
    "        \n",
    "        # Insert the new md5 hash into the metadata table\n",
    "        new_entry = [(layer_name, table_name, schema_flag, md5_hash)]\n",
    "        new_df = spark.createDataFrame(new_entry, [\"Layer_Name\", \"Table_Name\", \"Schema_Flag\", \"Md5_Hash\"])\n",
    "        new_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"Audit.Schema_Metadata\")\n",
    "        \n",
    "        # Since it's the first entry, return True\n",
    "        return True\n",
    "    \n",
    "    # If the metadata table exists, check if there's an entry for this layer and table\n",
    "    metadata_df = spark.table(\"Audit.Schema_Metadata\")\n",
    "    table_metadata = metadata_df.filter((col(\"Layer_Name\") == layer_name) & (col(\"Table_Name\") == table_name)).limit(1)\n",
    "    \n",
    "    if table_metadata.count() == 0:\n",
    "        # If no existing entry for this table, insert new data and return True\n",
    "        new_entry = [(layer_name, table_name, schema_flag, md5_hash)]\n",
    "        new_df = spark.createDataFrame(new_entry, [\"Layer_Name\", \"Table_Name\", \"Schema_Flag\", \"Md5_Hash\"])\n",
    "        new_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"Audit.Schema_Metadata\")\n",
    "        return True\n",
    "    else:\n",
    "        # Compare the MD5 hash from the metadata table with the new one\n",
    "        stored_md5 = table_metadata.first()[\"Md5_Hash\"]\n",
    "        stored_schema_flag = table_metadata.first()[\"Schema_Flag\"]\n",
    "        \n",
    "        if stored_md5 == md5_hash:\n",
    "            # If MD5 hash matches, return True\n",
    "            return True\n",
    "        else:\n",
    "            # If MD5 hash doesn't match, check the schema flag\n",
    "            if stored_schema_flag == \"N\":\n",
    "                # If schema_flag is \"N\", it's acceptable to have a different schema, return True\n",
    "                return True\n",
    "            elif stored_schema_flag == \"Y\":\n",
    "                # If schema_flag is \"Y\", it's not acceptable to have a different schema, return False\n",
    "                return False"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Audit_Table",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
